import nltk
from nltk.tokenize import word_tokenize
from nltk.util import ngrams

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Downloading 'punkt' tokenizer (once-off)...")
    nltk.download('punkt')

sentence = "The quick brown fox jumps over the lazy dog."
print(f"Original Sentence: {sentence}")

tokens = word_tokenize(sentence.lower())
print(f"\n--- Unigram (n=1) ---")
print(tokens)

bigrams = list(ngrams(tokens, 2))
print(f"\n--- Bigrams (n=2) ---")
print(bigrams)

trigrams = list(ngrams(tokens, 3))
print(f"\n--- Trigrams (n=3) ---")
print(trigrams)
